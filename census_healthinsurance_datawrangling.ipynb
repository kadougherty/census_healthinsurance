{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bc73b0",
   "metadata": {},
   "source": [
    "## predicting the health uninsured in the US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4e1cc",
   "metadata": {},
   "source": [
    "This project will call data from the American Community Survey Public Use MicroSample(PUMS) API. The most recent survey publicly availble at this API is from 2019. (Note data from 2020 are available to download in csv files).\n",
    "\n",
    "1. American Community Survey (ACS) (census.gov).\n",
    "https://www.census.gov/programs-surveys/acs/\n",
    "2. American Community Survey Data via API (census.gov).\n",
    "https://www.census.gov/programs-surveys/acs/data/data-via-api.html\n",
    "\n",
    "The goal of this project is to predict whether an individual has health insurance (or not) based on demographic data in the PUMS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7322b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a044184b",
   "metadata": {},
   "source": [
    "There are hundreds of variables in the PUMS dataset. Load in the variable names (code) and their descriptions (label) from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb68293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all variable names from census PUMS (public use microsample) API\n",
    "load_dotenv()\n",
    "API_KEY  = os.getenv('CENSUS_API_KEY')\n",
    "host     = 'https://api.census.gov/data'\n",
    "year     = 2019\n",
    "dataset  = 'acs/acs1/pums/variables'\n",
    "base_url = \"/\".join([host, str(year), dataset]) \n",
    "r        = requests.get(base_url)\n",
    "rlists   = r.json()\n",
    "code     = [item[0] for item in rlists if item[0].isupper()] #code in API\n",
    "label    = [item[1].lower() for item in rlists if item[0].isupper()] # description for each code\n",
    "nsamples = 500\n",
    "\n",
    "codedict = {} # redudant with code, label but may make life easier later on\n",
    "count    = 0\n",
    "for lab in label:\n",
    "    codedict[code[count]] = lab\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5002d5c",
   "metadata": {},
   "source": [
    "When calling the Census API, a maximum of 50 variables can be requested at one time. The data are also easily accessed state-by-state. The PUMS represents ~1% of the population. If we want to use data from all states, we could start by subsampling from availbable records. Set the number of samples (nsamples) in getCensusDataByState().\n",
    "\n",
    "One approach is to request data from a list of variables for a specific state in one call. The function callCensusAPI() below is meant to return a dataframe with variables in 'select_codes' from one state ('stateid'). A second function, getCensusDataByState() is meant to request data for all variables for one state in multiple calls and merge that data into one single dataframe for that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba650502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get data from Census API, Public Access MicroData Samples using list of codes (max 50), state id number \n",
    "def callCensusApi(API_KEY,year,select_codes,stateid):\n",
    "    host     = 'https://api.census.gov/data'\n",
    "    dataset  = 'acs/acs1/pums'\n",
    "    query    = '?get='\n",
    "    variable = ','.join(select_codes)\n",
    "    base_url = \"/\".join([host, str(year), dataset]) + query + variable + '&for=state:' + stateid + '&key=' + API_KEY\n",
    "    callbeg  = time.time()\n",
    "    r        = requests.get(base_url) \n",
    "    colnames = variable.split(',')\n",
    "    colnames.append('state')\n",
    "    df       = pd.DataFrame(columns=colnames, data=r.json()[1:]) \n",
    "    tElapsed = time.time()-callbeg\n",
    "    return (tElapsed,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbfc0277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get full dataframe (all variables in code) for one state. uses callCensusAPI function above\n",
    "def getCensusDataByState(state_id):\n",
    "    max_var_call = 50\n",
    "    start_var_n  = 0\n",
    "    end_var_n = start_var_n + max_var_call\n",
    "    totalTime = 0\n",
    "    while end_var_n <= len(code): \n",
    "        tElapsed, hdf = callCensusApi(API_KEY,year,code[start_var_n:end_var_n],state_id)\n",
    "        if start_var_n == 0:\n",
    "            df = hdf.sample(nsamples)\n",
    "            rowid = df.index\n",
    "            df = df.reset_index()\n",
    "        else:\n",
    "            sub = hdf.iloc[rowid]\n",
    "            sub = sub.reset_index()\n",
    "            df = pd.merge(df,sub,how='left',on='index')\n",
    "            del sub\n",
    "        totalTime = totalTime + tElapsed\n",
    "        start_var_n = end_var_n + 1\n",
    "        end_var_n = start_var_n + max_var_call\n",
    "    if df.shape[1] < len(code):\n",
    "        tElapsed, hdf = callCensusApi(API_KEY,year,code[start_var_n:len(code)],state_id)\n",
    "        sub = hdf.iloc[rowid]\n",
    "        sub = sub.reset_index()\n",
    "        df  = pd.merge(df,sub,how='left',on='index')      \n",
    "        totalTime = totalTime + tElapsed\n",
    "        del hdf\n",
    "    return (df,totalTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e118b8",
   "metadata": {},
   "source": [
    "With the functions above, we can loop through our states of interest and save a dataframe for each if it does not exist locally yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc39e1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already exists\n"
     ]
    }
   ],
   "source": [
    "datadir = os.path.join(os.getcwd(),\"data\")\n",
    "try:\n",
    "    os.mkdir(datadir)\n",
    "except:\n",
    "    print('already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c8e53cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kacie\\AppData\\Local\\Temp/ipykernel_2200/2399290971.py:16: FutureWarning: Passing 'suffixes' which cause duplicate columns {'state_x'} in the result is deprecated and will raise a MergeError in a future version.\n",
      "  df = pd.merge(df,sub,how='left',on='index')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state04: 129.3967263698578\n",
      "state05: 57.973498582839966\n",
      "state06: 705.9580404758453\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "[Errno Expecting value] : 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    336\u001b[0m         \"\"\"\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\json\\decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expecting value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2200/3433810302.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mfname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'state'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstate_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_subsample'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtotalTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetCensusDataByState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m# save the data to a new csv file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2200/2399290971.py\u001b[0m in \u001b[0;36mgetCensusDataByState\u001b[1;34m(state_id)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtotalTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mend_var_n\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtElapsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallCensusApi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAPI_KEY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_var_n\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_var_n\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart_var_n\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2200/1789620690.py\u001b[0m in \u001b[0;36mcallCensusApi\u001b[1;34m(API_KEY, year, select_codes, stateid)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mcolnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mcolnames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdf\u001b[0m       \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtElapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mcallbeg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtElapsed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    915\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: [Errno Expecting value] : 0"
     ]
    }
   ],
   "source": [
    "# call API for states in loop and save csv locally if it doesn't exist yet:\n",
    "n_states = 51\n",
    "for state in range(4,n_states+1):\n",
    "    state_id  =  f\"{state:02}\"\n",
    "    fname = os.path.join(datadir,'state' + state_id + '_subsample' + str(nsamples) + '.csv')\n",
    "    if ~os.path.exists(fname):\n",
    "        df,totalTime = getCensusDataByState(state_id)\n",
    "        # save the data to a new csv file\n",
    "        df.to_csv(fname)\n",
    "        print('state' + state_id + ': ' + str(totalTime))\n",
    "    del df\n",
    "    del fname\n",
    "    del state_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aae5be19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\kacie\\\\Documents\\\\ds\\\\census_healthinsurance\\\\data\\\\state01_subsample500.csv', 'C:\\\\Users\\\\kacie\\\\Documents\\\\ds\\\\census_healthinsurance\\\\data\\\\state02_subsample500.csv']\n"
     ]
    }
   ],
   "source": [
    "# load saved csv files:\n",
    "filenames = glob.glob(os.path.join(datadir,'state' + '*' + str(nsamples) + '.csv'))\n",
    "data_all = pd.concat((pd.read_csv(file) for file in filenames)).reset_index(drop = True) # Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c464789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the subsampled dataframe: 1000 by 518\n",
      "   Unnamed: 0  index  HHLANP  FBATHP  DRIVESP  WGTP23  WGTP22  WGTP25  WGTP24  \\\n",
      "0           0  35478    1200       0        1     167     164     241     130   \n",
      "1           1  41677    9500       0        1       8      19       6       8   \n",
      "2           2  48485    9500       0        0     153      78      19     113   \n",
      "3           3  25087    9500       0        0      71      61      21     124   \n",
      "4           4  43537    9500       0        1      19      62      71      78   \n",
      "\n",
      "   RACNH  ...  FDOUTP  PERNP  SCH  state_y.4  TEL  TEN  MLPI  MLPJ  MLPK  \\\n",
      "0      0  ...       0 -10001    2          1    1    2    -1    -1    -1   \n",
      "1      0  ...       0      0    2          1    1    3    -1    -1    -1   \n",
      "2      0  ...       0 -10001    2          1    1    1    -1    -1    -1   \n",
      "3      0  ...       0 -10001    2          1    1    3    -1    -1    -1   \n",
      "4      0  ...       0      0    2          1    1    1    -1    -1    -1   \n",
      "\n",
      "   state  \n",
      "0      1  \n",
      "1      1  \n",
      "2      1  \n",
      "3      1  \n",
      "4      1  \n",
      "\n",
      "[5 rows x 518 columns]\n"
     ]
    }
   ],
   "source": [
    "# check basic attributes of saved data:\n",
    "print('shape of the subsampled dataframe: ' + str(data_all.shape[0]) + ' by ' + str(data_all.shape[1]))\n",
    "print(data_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6bbd552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call api for more details on a specific variable\n",
    "def variableDetails(codename):\n",
    "    base_url = \"/\".join([host, str(year), dataset, codename + '.json']) \n",
    "    r        = requests.get(base_url)\n",
    "    detail   = r.json()\n",
    "    return detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a1ce21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keeping Unnamed: 0\n",
      "keeping index\n",
      "keeping state_x\n",
      "keeping state_y\n",
      "keeping state\n",
      "(1000, 510)\n"
     ]
    }
   ],
   "source": [
    "# check for extra columns that may have appeared from the dataframe merger:\n",
    "for column in data_all:\n",
    "    if column not in codedict.keys():\n",
    "        if column.__contains__('.'):\n",
    "            data_all.drop(column,axis=1,inplace=True)\n",
    "        else:\n",
    "            print('keeping ' + column)\n",
    "\n",
    "print(data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "635e7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the way we merged state_x and state_y should be redundant\n",
    "if data_all['state_x'].equals(data_all['state_y']):\n",
    "    data_all.drop('state_y',inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b854b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the state column should be incomplete based on the way we merged, drop if it has more missing values than 'state_x'\n",
    "if data_all['state'].isna().sum() > data_all['state_x'].isna().sum():\n",
    "    data_all.drop('state',axis=1,inplace=True)\n",
    "# drop Unnamed: 0 because it's not useful at this point (from old indices)\n",
    "data_all.drop('Unnamed: 0',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "516d52a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRIVCOV : private health insurance coverage recode\n",
      "HICOV : health insurance coverage recode\n",
      "FHICOVP : health insurance coverage recode allocation flag\n",
      "FPRIVCOVP : private health insurance coverage recode allocation flag\n"
     ]
    }
   ],
   "source": [
    "# check for target variable (health insured or not):\n",
    "for key, value in codedict.items():\n",
    "    if value.__contains__('health insurance'):\n",
    "        print(key + \" : \" + value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd41abc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from HICOV : \n",
      "2 : No health insurance coverage\n",
      "1 : With health insurance coverage\n",
      "from FHICOVP : \n",
      "0 : No\n",
      "1 : Yes\n"
     ]
    }
   ],
   "source": [
    "# let's check details on health insurance coverage recode, and \"...allocation flag\"\n",
    "codename = 'HICOV'\n",
    "details = variableDetails(codename)\n",
    "print('from ' + codename + ' : ')\n",
    "for key,value in details['values']['item'].items():\n",
    "    print(key, ':', value)\n",
    "\n",
    "codename = 'FHICOVP'\n",
    "print('from ' + codename + ' : ')\n",
    "details = variableDetails(codename)\n",
    "for key,value in details['values']['item'].items():\n",
    "    print(key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf991e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 508)\n",
      "(1000, 507)\n"
     ]
    }
   ],
   "source": [
    "# we can drop the HICOV column, since HICOV and FHICOVP have the same info\n",
    "print(data_all.shape)\n",
    "data_all.drop('FHICOVP',axis=1,inplace=True)\n",
    "print(data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac79a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.concat([data_all.isnull().sum(), 100 * data_all.isnull().mean()], axis=1)\n",
    "missing.columns=['count', '%']\n",
    "missing.sort_values(by='count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a0e3e4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OTHSVCEX : other internet service\n",
      "GASP : gas cost (monthly cost, use adjhsg to adjust gasp to constant dollars)\n",
      "WGTP11 : housing weight replicate 11\n",
      "MAR : marital status\n",
      "WGTP66 : housing weight replicate 66\n",
      "FHIMRKSP : subsidized marketplace coverage allocation flag\n",
      "HINS5 : tricare or other military health care\n",
      "FMRGXP : first mortgage status allocation flag\n",
      "FES : family type and employment status\n",
      "WKL : when last worked\n",
      "WGTP33 : housing weight replicate 33\n",
      "WGTP43 : housing weight replicate 43\n",
      "WGTP59 : housing weight replicate 59\n",
      "PWGTP27 : person's weight replicate 27\n",
      "PWGTP18 : person's weight replicate 18\n",
      "AGS : sales of agriculture products (yearly sales, no adjustment factor is applied)\n",
      "PWGTP62 : person's weight replicate 62\n",
      "PWGTP52 : person's weight replicate 52\n",
      "WGTP : housing unit weight\n",
      "WGTP62 : housing weight replicate 62\n",
      "WGTP80 : housing weight replicate 80\n",
      "BDSP : number of bedrooms\n",
      "FKITP : complete kitchen facilities allocation flag\n",
      "FHINS5C : tricare coverage given through the eligibility coverage edit\n",
      "FVALP : property value allocation flag\n",
      "FHINS1P : insurance through a current or former employer or union allocation flag\n",
      "FGCLP : grandparents living with grandchildren allocation flag\n",
      "FDREMP : cognitive difficulty allocation flag\n",
      "SCIENGP : field of degree science and engineering flag - nsf definition\n"
     ]
    }
   ],
   "source": [
    "# check for missing values:\n",
    "missing_values = []\n",
    "for column in data_all:\n",
    "    missing_values.append(data_all[column].isna().sum())\n",
    "    if data_all[column].isna().sum() > 0:\n",
    "        idx = code.index(column)\n",
    "        print(code[idx] + ' : ' + label[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
